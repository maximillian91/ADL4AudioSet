{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_classes=527\n",
    "feature_sizes=[128]\n",
    "feature_names=[\"audio_embedding\"]\n",
    "max_frames=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_and_decode(filename_queue):\n",
    "    reader = tf.TFRecordReader()\n",
    "    \n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    \n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        # Defaults are not specified since both keys are required.\n",
    "        features={\n",
    "            'video_id': tf.FixedLenFeature([], tf.string),\n",
    "            'start_time_seconds': tf.FixedLenFeature([], tf.float32),\n",
    "            'end_time_seconds': tf.FixedLenFeature([], tf.float32),\n",
    "            'labels': tf.VarLenFeature(tf.int64),\n",
    "            'audio_embedding': tf.FixedLenFeature([], tf.string)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # DECODE FEATURES\n",
    "    \n",
    "    # Video_id string\n",
    "    #video_id = tf.decode_raw(features['video_id'], tf.string)\n",
    "    \n",
    "    #start_time_seconds = tf.cast(features['start_time_seconds'], tf.float32)\n",
    "    #end_time_seconds = tf.cast(features['end_time_seconds'], tf.float32)\n",
    "    \n",
    "    # Convert label from a scalar int64 tensor to an int32 scalar.\n",
    "    labels = tf.cast(features['labels'], tf.int32)\n",
    "    #audio_embedding = tf.decode_raw(features['audio_embedding'], tf.float32)\n",
    "    \n",
    "    #audio_embedding_batch, \n",
    "    labels_batch = tf.train.shuffle_batch(\n",
    "        [labels],\n",
    "        batch_size=10,\n",
    "        capacity=30,\n",
    "        num_threads=10,\n",
    "        min_after_dequeue=10\n",
    "    )\n",
    "    \n",
    "    return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def resize_axis(tensor, axis, new_size, fill_value=0):\n",
    "    \"\"\"Truncates or pads a tensor to new_size on on a given axis.\n",
    "    Truncate or extend tensor such that tensor.shape[axis] == new_size. If the\n",
    "    size increases, the padding will be performed at the end, using fill_value.\n",
    "    Args:\n",
    "      tensor: The tensor to be resized.\n",
    "      axis: An integer representing the dimension to be sliced.\n",
    "      new_size: An integer or 0d tensor representing the new value for\n",
    "        tensor.shape[axis].\n",
    "      fill_value: Value to use to fill any new entries in the tensor. Will be\n",
    "        cast to the type of tensor.\n",
    "    Returns:\n",
    "      The resized tensor.\n",
    "    \"\"\"\n",
    "    tensor = tf.convert_to_tensor(tensor)\n",
    "    shape = tf.unstack(tf.shape(tensor))\n",
    "\n",
    "    pad_shape = shape[:]\n",
    "    pad_shape[axis] = tf.maximum(0, new_size - shape[axis])\n",
    "\n",
    "    shape[axis] = tf.minimum(shape[axis], new_size)\n",
    "    shape = tf.stack(shape)\n",
    "\n",
    "    resized = tf.concat([\n",
    "        tf.slice(tensor, tf.zeros_like(shape), shape),\n",
    "        tf.fill(tf.stack(pad_shape), tf.cast(fill_value, tensor.dtype))\n",
    "    ], axis)\n",
    "\n",
    "    # Update shape.\n",
    "    new_shape = tensor.get_shape().as_list()  # A copy is being made.\n",
    "    new_shape[axis] = new_size\n",
    "    resized.set_shape(new_shape)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_video_matrix(features,\n",
    "                      feature_size,\n",
    "                      smax_frames,\n",
    "                      max_quantized_value,\n",
    "                      min_quantized_value):\n",
    "    \"\"\"Decodes features from an input string and quantizes it.\n",
    "    Args:\n",
    "      features: raw feature values\n",
    "      feature_size: length of each frame feature vector\n",
    "      max_frames: number of frames (rows) in the output feature_matrix\n",
    "      max_quantized_value: the maximum of the quantized value.\n",
    "      min_quantized_value: the minimum of the quantized value.\n",
    "    Returns:\n",
    "      feature_matrix: matrix of all frame-features\n",
    "      num_frames: number of frames in the sequence\n",
    "    \"\"\"\n",
    "    decoded_features = tf.reshape(\n",
    "        tf.cast(tf.decode_raw(features, tf.uint8), tf.float32),\n",
    "        [-1, feature_size])\n",
    "\n",
    "    num_frames = tf.minimum(tf.shape(decoded_features)[0], max_frames)\n",
    "    feature_matrix = utils.Dequantize(decoded_features,\n",
    "                                      max_quantized_value,\n",
    "                                      min_quantized_value)\n",
    "    feature_matrix = resize_axis(feature_matrix, 0, max_frames)\n",
    "    #print feature_matrix, feature_matrix.shape, num_frames\n",
    "    #sys.exit()\n",
    "    return feature_matrix, num_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prepare_serialized_examples(serialized_example,\n",
    "    max_quantized_value=2, min_quantized_value=-2):\n",
    "\n",
    "    contexts, features = tf.parse_single_sequence_example(\n",
    "        serialized_example,\n",
    "        context_features={\"video_id\": tf.FixedLenFeature(\n",
    "            [], tf.string),\n",
    "                          \"labels\": tf.VarLenFeature(tf.int64)},\n",
    "        sequence_features={\n",
    "            feature_name : tf.FixedLenSequenceFeature([], dtype=tf.string)\n",
    "            for feature_name in feature_names\n",
    "        })\n",
    "\n",
    "    # read ground truth labels\n",
    "    labels = (tf.cast(\n",
    "        tf.sparse_to_dense(contexts[\"labels\"].values, (num_classes,), 1,\n",
    "            validate_indices=False),\n",
    "        tf.bool))\n",
    "\n",
    "    # loads (potentially) different types of features and concatenates them\n",
    "    num_features = len(feature_names)\n",
    "    #print num_features\n",
    "    assert num_features > 0, \"No feature selected: feature_names is empty!\"\n",
    "\n",
    "    assert len(feature_names) == len(feature_sizes), \\\n",
    "    \"length of feature_names (={}) != length of feature_sizes (={})\".format( \\\n",
    "    len(feature_names), len(feature_sizes))\n",
    "\n",
    "    num_frames = -1  # the number of frames in the video\n",
    "    feature_matrices = [None] * num_features  # an array of different features\n",
    "    for feature_index in range(num_features):\n",
    "      #print feature_index\n",
    "      feature_matrix, num_frames_in_this_feature = get_video_matrix(\n",
    "          features[feature_names[feature_index]],\n",
    "          feature_sizes[feature_index],\n",
    "          max_frames,\n",
    "          max_quantized_value,\n",
    "          min_quantized_value)\n",
    "    if num_frames == -1:\n",
    "        num_frames = num_frames_in_this_feature\n",
    "    else:\n",
    "        tf.assert_equal(num_frames, num_frames_in_this_feature)\n",
    "\n",
    "    feature_matrices[feature_index] = feature_matrix\n",
    "\n",
    "    # cap the number of frames at self.max_frames\n",
    "    num_frames = tf.minimum(num_frames, max_frames)\n",
    "\n",
    "    # concatenate different features\n",
    "    video_matrix = tf.concat(feature_matrices, 1)\n",
    "\n",
    "    # convert to batch format.\n",
    "    video_id=contexts[\"video_id\"]\n",
    "    \n",
    "    # TODO: Do proper batch reads to remove the IO bottleneck.\n",
    "    video_id_batch, video_matrix_batch, labels_batch, num_frames_batch = tf.train.shuffle_batch(\n",
    "        [video_id, video_matrix, labels, num_frames],\n",
    "        batch_size=10,\n",
    "        capacity=30,\n",
    "        num_threads=5,\n",
    "        min_after_dequeue=10\n",
    "    )\n",
    "    #batch_video_ids = tf.expand_dims(contexts[\"video_id\"], 0)\n",
    "    #batch_video_matrix = tf.expand_dims(video_matrix, 0)\n",
    "    #batch_labels = tf.expand_dims(labels, 0)\n",
    "    #batch_frames = tf.expand_dims(num_frames, 0)\n",
    "\n",
    "    #return batch_video_ids, batch_video_matrix, batch_labels, batch_frames\n",
    "    return video_id_batch, video_matrix_batch, labels_batch, num_frames_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"shuffle_batch_3:0\", shape=(10,), dtype=string)\n",
      "Tensor(\"shuffle_batch_3:1\", shape=(10, 300, 128), dtype=float32)\n",
      "Tensor(\"shuffle_batch_3:2\", shape=(10, 527), dtype=bool)\n",
      "Tensor(\"shuffle_batch_3:3\", shape=(10,), dtype=int32)\n",
      "(10,)\n",
      "[b'kWjt5M1_xbY' b'MKwnxr3ypB4' b'MKM3Rs5Kg8c' b'MbOu1rlgL3U' b'MK1NzQm9XaQ'\n",
      " b'UlqBFf63wXE' b'9332PjiYkhI' b'MKqXe4X6g-8' b'kWTuTNmyers' b'93b0scnqSOw']\n",
      "(10, 527)\n",
      "[[False False False ..., False False False]\n",
      " [ True False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " ..., \n",
      " [ True False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]]\n",
      "(10, 300, 128)\n",
      "[[-0.26669717  0.45487142  0.98820472 ..., -1.9921875  -0.09414816\n",
      "  -0.32944226]\n",
      " [-0.4235599   0.23526359  0.0470283  ..., -1.9921875   2.0078125\n",
      "  -0.03140306]\n",
      " [-0.98826587  0.81565571 -0.07846189 ..., -0.9725796   2.0078125\n",
      "   0.48624396]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " current batch\n",
      "(10,)\n",
      "[b'I2FMmelhjwI' b'vcj4kacdmds' b'vcVDYzew53A' b'hDLkrQvmvd0' b'MbKkOpo09iI'\n",
      " b'UlVvWsYNy44' b'93vYcBbPDwQ' b'S4bUmXQ7Bis' b'MbXLrYf-lK4' b'I2FnnsvMj6Y']\n",
      "(10, 527)\n",
      "[[False False False ..., False False False]\n",
      " [ True False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " ..., \n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " [ True False False ..., False False False]]\n",
      "(10, 300, 128)\n",
      "[[ 1.09800887 -0.83140314 -0.53336394 ...,  0.95683217 -1.25493264\n",
      "  -1.4588542 ]\n",
      " [ 1.16075397 -0.75297177 -0.29806972 ..., -0.64316785 -0.86277568\n",
      "  -0.12552071]\n",
      " [ 1.12938142 -0.69022667 -0.32944226 ...,  0.12545967 -0.4235599\n",
      "   1.03526378]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " current batch\n",
      "(10,)\n",
      "[b'hD1u-7ROwZg' b'HDFGeKZTymw' b'AJtOE_JSAhw' b'S42yUMzpiMk' b'I2ZLX89ycjc'\n",
      " b'AJROvxlmo40' b'HDDxzqEUy74' b'S48GsFznk50' b'Rvd3kZDrTss' b'I2FKGwZ9oMs']\n",
      "(10, 527)\n",
      "[[False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " [ True False False ..., False False False]\n",
      " ..., \n",
      " [False False False ..., False False False]\n",
      " [ True False False ..., False False False]\n",
      " [False False False ..., False False False]]\n",
      "(10, 300, 128)\n",
      "[[ 0.5803616  -0.64316785  0.37644005 ..., -0.40787363 -0.36081481\n",
      "  -0.92552078]\n",
      " [ 0.78428316 -0.84708941  0.37644005 ...,  1.58428335 -0.50199139\n",
      "  -1.06669724]\n",
      " [ 0.61173415 -0.40787363 -0.80003059 ..., -0.26669717  0.26663613\n",
      "  -0.78434432]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " current batch\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Run call was cancelled\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "bal_train_dir = '../data/audioset_v1_embeddings/bal_train/' \n",
    "directory = os.fsencode(bal_train_dir)\n",
    "\n",
    "tfrecord_files = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".tfrecord\"): \n",
    "        file_path = os.path.join(os.fsdecode(directory), filename)\n",
    "        tfrecord_files.append(file_path)\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "    tfrecord_files,\n",
    "    num_epochs=10\n",
    ")\n",
    "\n",
    "reader = tf.TFRecordReader()\n",
    "\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "video_id, video_matrix, labels, num_frames = prepare_serialized_examples(\n",
    "    serialized_example\n",
    ")\n",
    "\n",
    "print(video_id)\n",
    "print(video_matrix)\n",
    "print(labels)\n",
    "print(num_frames)\n",
    "\n",
    "\n",
    "# The op for initializing the variables.\n",
    "init_op = tf.group(tf.global_variables_initializer(),\n",
    "                   tf.local_variables_initializer())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    # Let's read off 3 batches just for example\n",
    "    for i in range(3):\n",
    "    \n",
    "        vid, vidmat, lab= sess.run([video_id, video_matrix, labels])\n",
    "        print(vid.shape)\n",
    "        print(vid)\n",
    "        print(lab.shape)\n",
    "        print(lab)\n",
    "        print(vidmat.shape)\n",
    "        print(vidmat[0])\n",
    "        \n",
    "        print('\\n current batch')\n",
    "\n",
    "#labels_batch = read_and_decode(filename_queue)\n",
    "\n",
    "#print(labels_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
