{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "feature_names = ['audio_embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_decode(filename_queue):\n",
    "    reader = tf.TFRecordReader()\n",
    "    \n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    \n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        # Defaults are not specified since both keys are required.\n",
    "        features={\n",
    "            'video_id': tf.FixedLenFeature([], tf.string),\n",
    "            'start_time_seconds': tf.FixedLenFeature([], tf.float32),\n",
    "            'end_time_seconds': tf.FixedLenFeature([], tf.float32),\n",
    "            'labels': tf.VarLenFeature(tf.int64),\n",
    "            'audio_embedding': tf.FixedLenFeature([], tf.string)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # DECODE FEATURES\n",
    "    \n",
    "    # Video_id string\n",
    "    #video_id = tf.decode_raw(features['video_id'], tf.string)\n",
    "    \n",
    "    #start_time_seconds = tf.cast(features['start_time_seconds'], tf.float32)\n",
    "    #end_time_seconds = tf.cast(features['end_time_seconds'], tf.float32)\n",
    "    \n",
    "    # Convert label from a scalar int64 tensor to an int32 scalar.\n",
    "    labels = tf.cast(features['labels'], tf.int32)\n",
    "    #audio_embedding = tf.decode_raw(features['audio_embedding'], tf.float32)\n",
    "    \n",
    "    #audio_embedding_batch, \n",
    "    labels_batch = tf.train.shuffle_batch(\n",
    "        [labels],\n",
    "        batch_size=10,\n",
    "        capacity=30,\n",
    "        num_threads=10,\n",
    "        min_after_dequeue=10\n",
    "    )\n",
    "    \n",
    "    return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_serialized_examples(serialized_example,\n",
    "    max_quantized_value=2, min_quantized_value=-2):\n",
    "\n",
    "    contexts, features = tf.parse_single_sequence_example(\n",
    "        serialized_example,\n",
    "        context_features={\"video_id\": tf.FixedLenFeature(\n",
    "            [], tf.string),\n",
    "                          \"labels\": tf.VarLenFeature(tf.int64)},\n",
    "        sequence_features={\n",
    "            feature_name : tf.FixedLenSequenceFeature([], dtype=tf.string)\n",
    "            for feature_name in feature_names\n",
    "        })\n",
    "\n",
    "    # read ground truth labels\n",
    "    labels = (tf.cast(\n",
    "        tf.sparse_to_dense(contexts[\"labels\"].values, (num_classes,), 1,\n",
    "            validate_indices=False),\n",
    "        tf.bool))\n",
    "\n",
    "    # loads (potentially) different types of features and concatenates them\n",
    "    num_features = len(feature_names)\n",
    "    #print num_features\n",
    "    assert num_features > 0, \"No feature selected: feature_names is empty!\"\n",
    "\n",
    "    assert len(feature_names) == len(feature_sizes), \\\n",
    "    \"length of feature_names (={}) != length of feature_sizes (={})\".format( \\\n",
    "    len(feature_names), len(feature_sizes))\n",
    "\n",
    "    num_frames = -1  # the number of frames in the video\n",
    "    feature_matrices = [None] * num_features  # an array of different features\n",
    "    for feature_index in range(num_features):\n",
    "      #print feature_index\n",
    "      feature_matrix, num_frames_in_this_feature = get_video_matrix(\n",
    "          features[feature_names[feature_index]],\n",
    "          feature_sizes[feature_index],\n",
    "          max_frames,\n",
    "          max_quantized_value,\n",
    "          min_quantized_value)\n",
    "    if num_frames == -1:\n",
    "        num_frames = num_frames_in_this_feature\n",
    "    else:\n",
    "        tf.assert_equal(num_frames, num_frames_in_this_feature)\n",
    "\n",
    "    feature_matrices[feature_index] = feature_matrix\n",
    "\n",
    "    # cap the number of frames at self.max_frames\n",
    "    num_frames = tf.minimum(num_frames, max_frames)\n",
    "\n",
    "    # concatenate different features\n",
    "    video_matrix = tf.concat(feature_matrices, 1)\n",
    "\n",
    "    # convert to batch format.\n",
    "    video_id=contexts[\"video_id\"]\n",
    "    # TODO: Do proper batch reads to remove the IO bottleneck.\n",
    "    #batch_video_ids = tf.expand_dims(contexts[\"video_id\"], 0)\n",
    "    #batch_video_matrix = tf.expand_dims(video_matrix, 0)\n",
    "    #batch_labels = tf.expand_dims(labels, 0)\n",
    "    #batch_frames = tf.expand_dims(num_frames, 0)\n",
    "\n",
    "    #return batch_video_ids, batch_video_matrix, batch_labels, batch_frames\n",
    "    return video_id, video_matrix, labels, num_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.data_flow_ops.FIFOQueue object at 0x7fe62684a470>\n",
      "SparseTensor(indices=Tensor(\"ParseSingleExample_18/Slice_Indices_labels:0\", shape=(?, 1), dtype=int64), values=Tensor(\"Cast_15:0\", shape=(?,), dtype=int32), dense_shape=Tensor(\"ParseSingleExample_18/Squeeze_Shape_labels:0\", shape=(1,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "bal_train_dir = '../data/audioset_v1_embeddings/bal_train/' \n",
    "directory = os.fsencode(bal_train_dir)\n",
    "\n",
    "tfrecord_files = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\".tfrecord\"): \n",
    "        file_path = os.path.join(os.fsdecode(directory), filename)\n",
    "        # print(file_path)\n",
    "        tfrecord_files.append(file_path)\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "     \n",
    "\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "    tfrecord_files,\n",
    "    num_epochs=10\n",
    ")\n",
    "\n",
    "print(filename_queue)\n",
    "\n",
    "labels_batch = read_and_decode(filename_queue)\n",
    "\n",
    "print(labels_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
